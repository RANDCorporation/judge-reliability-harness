
# Making Cost Curves

**Definition**: A **cost curve** is a map {autograder, test_name} -> metric score.

Making cost curves is a complicated process. It requires setting your config file and running 
the JRH to generate the data you need. Here, we will walk through an example of the **label_flip**
test for two aiautograders: openai/gpt-4o-mini and anthropic/claude-3.

* Suppose your config file is here: ```./inputs/configs/custom_config.yml```

## Step 1: Generate synthetic data and evaluate using the first aiautograder.

In your config, you'll want to set:

```
admin:
  perturbation_config:
    tests_to_run:
      - "label_flip"

  evaluation_config:
    autograder_model_name: "openai/gpt-4o-mini"
    tests_to_evaluate:
      - "label_flip"
    get_cost_curves: False
```      

Run with ```python -m main ./inputs/configs/custom_config.yml```


## Step 2: Generate synthetic data and evaluate using the second aiautograder.

In your config, you'll want to set:

```
admin:
  perturbation_config:
    tests_to_run:
      - "label_flip"

  evaluation_config:
    autograder_model_name: "anthropic/claude-3"
    tests_to_evaluate:
      - "label_flip"
    get_cost_curves: False
```      

Run with ```python -m main ./inputs/configs/custom_config.yml```



## Step 3: Check cost curves.

In your config, you'll want to set:

```
admin:
  perturbation_config:
    tests_to_run:

  evaluation_config:
    autograder_model_name: "litellm/llama-2-7b-chat"
    tests_to_evaluate:
    get_cost_curves: True
```      

Run with ```python -m main ./inputs/configs/custom_config.yml```

You should get an output that looks like this at this location:
```./outputs/{PROJECT_NAME}_{TIMESTAMP}/cost_curve_heatmap.png```

![cost curve example](./walkthrough_figs/example_cost_curve_heatmap.png)

# Common Mistakes / Debugging

The cost curve is generated by taking the latest autograder for each aiautograder. So if you have:
* ```./outputs/harmbench_binary_20251921_1109/litellm_llama-2-7b-chat_report.json``` <- _ignored_
* ```./outputs/harmbench_binary_20251921_1115/litellm_llama-2-7b-chat_report.json``` <- _used for cost curve_

It may be the case that your "tests_to_evaluate" still had **label_flip** activated,
which generated a new report. Make sure this is empty.


