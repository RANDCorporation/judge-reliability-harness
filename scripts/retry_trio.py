"""
TEMPORARY script to evaluate LLM juries on an XLSX with rubric/transcript and true_label.
The script will be worked into the main harness soon, as time permits.
"""

#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import List, Sequence, Tuple

import numpy as np
import pandas as pd

_REPO_ROOT = Path(__file__).resolve().parents[1]
if str(_REPO_ROOT) not in sys.path:
	sys.path.insert(0, str(_REPO_ROOT))

from scripts.judge_eval import _compute_metrics, _parse_true_label  # noqa: E402


def _prediction_column_name(model: str) -> str:
	return f"pred_{model.replace('/', '_')}"


def _load_predictions(
	predictions_path: Path,
	label_col: str,
	models: Sequence[str],
) -> Tuple[np.ndarray, np.ndarray]:
	df = pd.read_csv(predictions_path)
	if label_col not in df.columns:
		raise ValueError(f"Missing label column '{label_col}' in {predictions_path}")

	missing_cols = [
		_prediction_column_name(model) for model in models if _prediction_column_name(model) not in df.columns
	]
	if missing_cols:
		raise ValueError(f"Missing prediction columns: {missing_cols}")

	y_true = df[label_col].apply(_parse_true_label).astype(int).to_numpy(dtype=int)
	model_cols = [_prediction_column_name(model) for model in models]
	mat = df[model_cols].apply(pd.to_numeric, errors="coerce").fillna(0).to_numpy(dtype=int)
	y_pred = (mat.sum(axis=1) >= 2).astype(int)
	return y_true, y_pred


def parse_args() -> argparse.Namespace:
	parser = argparse.ArgumentParser(
		description="Recompute trio majority metrics from an existing predictions.csv file.",
	)
	parser.add_argument(
		"--predictions",
		type=Path,
		required=True,
		help="Path to predictions.csv generated by judge_eval.",
	)
	parser.add_argument(
		"--models",
		type=str,
		nargs=3,
		default=["google/gemini-2.5-flash", "openai/gpt-5-nano", "openai/gpt-4.1"],
		# default=["anthropic/claude-opus-4-1", "google/gemini-2.5-flash", "openai/gpt-4.1-mini"],
		help="Exactly three model identifiers to recompute trio majority vote for.",
	)
	parser.add_argument(
		"--label-col",
		type=str,
		default="validation_score",
		help="Column that stores the ground-truth labels in predictions.csv.",
	)
	parser.add_argument(
		"--output-json",
		type=Path,
		default=None,
		help="Optional path to write the metrics JSON payload.",
	)
	return parser.parse_args()


def main() -> None:
	args = parse_args()
	predictions_path: Path = args.predictions
	models: List[str] = list(args.models)
	label_col: str = args.label_col
	output_json: Path | None = args.output_json

	y_true, y_pred = _load_predictions(predictions_path, label_col, models)
	metrics = _compute_metrics(y_true, y_pred)

	result = {
		"predictions_csv": str(predictions_path),
		"models": models,
		"metrics": metrics,
	}
	print(json.dumps(result, indent=2))

	if output_json:
		output_json.parent.mkdir(parents=True, exist_ok=True)
		with output_json.open("w", encoding="utf-8") as handle:
			json.dump(result, handle, indent=2)


if __name__ == "__main__":
	main()
