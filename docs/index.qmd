---
title: "Judge Reliability Harness Overview"
---

# Project Overview

The Judge Reliability Harness (JRH) orchestrates end-to-end evaluations of automated judges. It standardizes how datasets are prepared, perturbations are generated, and model outputs are rescored so teams can compare judge reliability across tasks. The harness coordinates data ingestion, perturbation pipelines, automated grading, and reporting into reproducible runs.

# Core Workflow

1. Load configuration describing the evaluation module, input assets, and runtime parameters.
2. Build perturbation and evaluation registries that determine which reliability tests execute.
3. Generate synthetic variants when required and persist the merged configuration for auditing.
4. Run harness execution to score perturbations, summarize performance, and emit artifacts inside `outputs/`.

![JRH Process overview flow](JRH_Process.png){fig-align="center" width="90%"}

# Available Modes

- `Open-ended Judge`: Binary grading for free-text responses.
- `Open-ended Autograder`: Ordinal grading for free-text responses.
- `Agentic Judge`: Agent transcript grading for binary criteria.
- `Agentic Autograder`: Agent transcript grading for ordinal criteria.

![Non-agentic and agentic control flow](ControlFlowDiagamBothModes.png){fig-align="center" width="100%"}

# Documentation Map

- `installation.qmd` covers prerequisites, uv setup, and required credentials.
- `quickstart.qmd` shows common run commands, how to toggle the review UI, and where outputs land.
- `user_guide.qmd` documents configuration fields, review UI behavior, outputs, and rerun expectations.
- `reliability_tests.qmd` details each synthetic reliability test and when to use it.
- `agentic_guide.qmd` explains the Inspect-log-driven agent perturbation workflow.
- `developer_guide.qmd` outlines the project layout for contributors.
